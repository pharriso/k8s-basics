As already highlighted, a key resource created when deploying an application is the `deployment` resource. It specifies the name of the container image to be deployed for an application, how many instances should be started, and the strategy for how the deployment should be managed.

To view the `deployment` resource used for the front end web application, run:

[.console-input]
[source, execute]
----
cat frontend/deployment-parksmap.yaml
----


This has various parts to it, as well as dependencies on other resources, so let's start over and create this from scratch.

To create a new `deployment` resource what often happens is that a developer will copy an existing one, be it one from an existing application, or a sample provided in documentation or a blog post.

An alternative is to have `kubectl` create it for you using the `kubectl create deployment` command.

[.console-input]
[source, execute]
----
kubectl create deployment --help
----

For the deployment of the front end web application, the container image we want to use is `quay.io/openshiftroadshow/parksmap:latest'`.

To see what `kubectl create deployment` would create for us run:

[.console-input]
[source, execute]
----
kubectl create deployment parksmap --image quay.io/openshiftroadshow/parksmap:latest --replicas 1 --dry-run=client -o yaml
----

This should yield:

[.console-output]
[source]
----
apiVersion: apps/v1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    app: parksmap
  name: parksmap
spec:
  replicas: 1
  selector:
    matchLabels:
      app: parksmap
  strategy: {}
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: parksmap
    spec:
      containers:
      - image: quay.io/openshiftroadshow/parksmap:latest
        name: parksmap
        resources: {}
status: {}
----

NOTE: In the previous module, we discussed the value of labels. Looking at the output, you'll notice that labels discussed such as 'role=frontend' are not there. The 'kubectl create deployment' command doesn't allow us to add labels. We'll add those in another step shortly. 

Nothing has been created at this point as when we ran `kubectl create deployment` we used the `--dry-run=client` option, it therefore only showed what it would create.

Now, run the command for real. 

[.console-input]
[source, execute]
----
kubectl create deployment parksmap --image quay.io/openshiftroadshow/parksmap:latest --replicas 1
----

You should see the following output:

[.console-output]
[source]
----
deployment.apps/parksmap created
----

As we have done previously, you can monitor the deployment to make sure it's successful by running the following command

[.console-input]
[source, execute]
----
kubectl rollout  status deployment/parksmap
----

To make the application more manageable we'll add our labels. Hopefully we'll get this simialar to those we deployed earlier.

Run the following command

[.console-input]
[source, execute]
----
kubectl label --overwrite deployment parksmap app=workshop app.kubernetes.io/part-of=workshop app.kubernetes.io/instance=parksmap app.kubernetes.io/component=parksmap app.openshift.io/runtime=rh-spring-boot role=frontend app.kubernetes.io/name=parksmap component=parksmap
----

To make sure the command was successful, run the following command

[.console-input]
[source, execute]
----
kubectl get  deployment parksmap -o yaml
----

The outout should look similar to the following:

[.console-output]
[source]
----
apiVersion: apps/v1
kind: Deployment
metadata:
  annotations:
    deployment.kubernetes.io/revision: "1"
  creationTimestamp: "2025-08-14T14:41:56Z"
  generation: 1
  labels:
    app: workshop
    app.kubernetes.io/component: parksmap
    app.kubernetes.io/instance: parksmap
    app.kubernetes.io/name: parksmap
    app.kubernetes.io/part-of: workshop
    app.openshift.io/runtime: rh-spring-boot
    component: parksmap
    role: frontend
  name: parksmap
  namespace: wksp-user1
  resourceVersion: "909871"
  uid: 67c938ee-dd95-4f3a-ab7c-40e21df9276b
spec:
  progressDeadlineSeconds: 600
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app: parksmap
  strategy:
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 25%
    type: RollingUpdate
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: parksmap
    spec:
      containers:
      - image: quay.io/openshiftroadshow/parksmap:latest
        imagePullPolicy: Always
        name: parksmap
        resources: {}
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      terminationGracePeriodSeconds: 30
...
----

You can see the labels you added above.

If you look closely, you might notice that there are some more labels in the above yaml. These appear after the 'template' line. 

The template section of the yaml file is used by Kubernetes to ensure that the number of pods executing is that defined in the deployment.  This process is automatic. Kubernetes uses the template section to  automatially create a `replicaset` from the template section. In turn the replicaset creates the correct number of Pods (more later). 

It would be a good idea if we had some of our labels defined in the template as well. To do this we'll use the `kubectl edit` command. 'kubectl edit' allows us to update the yaml files in Kubernetes directly. In your terminal window type:

[.console-input]
[source, execute]
----
kubectl edit deployment parksmap
----

You'll need to follow along carefully as the indentation is really important in yaml. If you are unfamilar with terminal line editors then please ask one of the Red Hatters to help.

You should see the following output:

[.console-output] 
[source]
----
# Please edit the object below. Lines beginning with a '#' will be ignored,
# and an empty file will abort the edit. If an error occurs while saving this file will be
# reopened with the relevant failures.
#
apiVersion: apps/v1
kind: Deployment
metadata:
  annotations:
    deployment.kubernetes.io/revision: "1"
  creationTimestamp: "2025-08-14T14:41:56Z"
  generation: 1
  labels:
    app: workshop
    app.kubernetes.io/component: parksmap
    app.kubernetes.io/instance: parksmap
    app.kubernetes.io/name: parksmap
    app.kubernetes.io/part-of: workshop
    app.openshift.io/runtime: rh-spring-boot
    component: parksmap
    role: frontend
  name: parksmap
  namespace: wksp-user1
  resourceVersion: "935191"
  uid: 67c938ee-dd95-4f3a-ab7c-40e21df9276b
spec:
  progressDeadlineSeconds: 600
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app: parksmap
  strategy:
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 25%
    type: RollingUpdate
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: parksmap
    spec:
      containers:
      - image: quay.io/openshiftroadshow/parksmap:latest
        imagePullPolicy: Always
        name: parksmap
        resources: {}
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      terminationGracePeriodSeconds: 30
status:
  availableReplicas: 1
  conditions:
  - lastTransitionTime: "2025-08-14T14:41:56Z"
    lastUpdateTime: "2025-08-14T14:41:59Z"
    message: ReplicaSet "parksmap-86f598b786" has successfully progressed.
----

Find the section under template called labels, you should see `app: parksmap` there.

[.console-input]
[source, execute]
----
delete that whole line 
----

copy and paste the following labels:

[.console-input]
[source, execute]
----
        app: parksmap
        component: parksmap
        deployment: parksmap
        role: frontend
----

`it is really important that they are indented by 2 spaces underneath labels`

When completed, the parksmap template label section should look like:

[.console-output]
[source]
----
# Please edit the object below. Lines beginning with a '#' will be ignored,
# and an empty file will abort the edit. If an error occurs while saving this file will be
# reopened with the relevant failures.
#
apiVersion: apps/v1
kind: Deployment
metadata:
  annotations:
    deployment.kubernetes.io/revision: "1"
  creationTimestamp: "2025-08-14T14:41:56Z"
  generation: 1
  labels:
    app: workshop
    app.kubernetes.io/component: parksmap
    app.kubernetes.io/instance: parksmap
    app.kubernetes.io/name: parksmap
    app.kubernetes.io/part-of: workshop
    app.openshift.io/runtime: rh-spring-boot
    component: parksmap
    role: frontend
  name: parksmap
  namespace: wksp-user1
  resourceVersion: "935191"
  uid: 67c938ee-dd95-4f3a-ab7c-40e21df9276b
spec:
  progressDeadlineSeconds: 600
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app: parksmap
  strategy:
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 25%
    type: RollingUpdate
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: parksmap
        component: parksmap
        deployment: parksmap
        role: frontend
...
----


Using 'kubectl create' to create the deployment is really useful, and a great tool for learning and creating the required yaml for all artefacts in the cluster.

Once you start heading to production though, you'll be wanting to automate deployment by creating, and deploying the yaml files (as we did in the earlier lab). Using tools such as Helm and Kustomize can make light work of templating/deploying applications into production.  

If you choose to store the yaml files into a Git repository, you can then starting thinking about gitops. Tools such as ArgoCD can be used to manage and deploy applications into production based on git repositories, and the changes that occur to the files held there.
