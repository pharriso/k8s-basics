As already highlighted, a key resource created when deploying an application is the `deployment` resource. It specifies the name of the container image to be deployed for an application, how many instances should be started, and the strategy for how the deployment should be managed.

To view the `deployment` resource used for the front end web application, run:

[.console-input]
[source, execute]
----
cat frontend/deployment-parksmap.yaml
----


This has various parts to it, as well as dependencies on other resources, so let's start over and create this from scratch.

To create a new `deployment` resource what often happens is that a developer will copy an existing one, be it one from an existing application, or a sample provided in documentation or a blog post.

An alternative is to have `kubectl` create it for you. For a `deployment`, the `kubectl` command provides two options for creating the resource definition for you.

The first option is using the `kubectl create deployment` command.

[.console-input]
[source, execute]
----
kubectl create deployment --help
----

For the deployment of the front end web application, the container image we want to use is `quay.io/openshiftroadshow/parksmap:latest'`.

To see what `kubectl create deployment` would create for us run:

[.console-input]
[source, execute]
----
kubectl create deployment parksmap --image quay.io/openshiftroadshow/parksmap:latest --replicas 1 --dry-run=client -o yaml
----

This should yield:

[.console-output]
[source]
----
apiVersion: apps/v1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    app: parksmap
  name: parksmap
spec:
  replicas: 1
  selector:
    matchLabels:
      app: parksmap
  strategy: {}
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: parksmap
    spec:
      containers:
      - image: quay.io/openshiftroadshow/parksmap:latest
        name: parksmap
        resources: {}
status: {}
----

NOTE: In the previous module, we discussed the value of labels. Looking at the output, you'll notice that labels discussed such as 'role=frontend' are not there. The 'kubectl create deployment' command doesn't allow us to add labels. We'll add those in another step shortly. 

Nothing has been created at this point as when we ran `kubectl create deployment` we used the `--dry-run=client` option, it therefore only showed what it would create.

Now, run the command for real. 

[.console-input]
[source, execute]
----
kubectl create deployment parksmap --image quay.io/openshiftroadshow/parksmap:latest --replicas 1
----

You should see the following output:

[.console-output]
[source]
----
deployment.apps/parksmap created
----

As we have done previously, you can monitor the deployment to make sure it's successful by running the following command

[.console-input]
[source, execute]
----
kubectl rollout  status deployment/parksmap
----

To make the application more manageable we'll add our labels. Hopefully we'll get his simialar to those we deployed earlier.

Run the following command

[.console-input]
[source, execute]
----
kubectl label --overwrite deployment parksmap app=workshop app.kubernetes.io/part-of=workshop app.kubernetes.io/instance=parksmap app.kubernetes.io/component=parksmap app.openshift.io/runtime=rh-spring-boot role=frontend app.kubernetes.io/name=parksmap component=parksmap
----

To make sure the command was successful, run the following command

[.console-input]
[source, execute]
----
kubectl describe deployment parksmap
----

The outout should look similar to the following:

[.console-output]
[source]
----
Name:                   parksmap
Namespace:              wksp-user1
CreationTimestamp:      Thu, 14 Aug 2025 14:41:56 +0000
Labels:                 app=workshop
                        app.kubernetes.io/component=parksmap
                        app.kubernetes.io/instance=parksmap
                        app.kubernetes.io/name=parksmap
                        app.kubernetes.io/part-of=workshop
                        app.openshift.io/runtime=rh-spring-boot
                        component=parksmap
                        role=frontend
Annotations:            deployment.kubernetes.io/revision: 1
Selector:               app=parksmap
Replicas:               1 desired | 1 updated | 1 total | 1 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  app=parksmap
  Containers:
   parksmap:
    Image:         quay.io/openshiftroadshow/parksmap:latest
    Port:          <none>
    Host Port:     <none>
    Environment:   <none>
    Mounts:        <none>
  Volumes:         <none>
  Node-Selectors:  <none>
  Tolerations:     <none>
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  <none>
NewReplicaSet:   parksmap-86f598b786 (1/1 replicas created)
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  51s   deployment-controller  Scaled up replica set parksmap-86f598b786 from 0 to 1
----

You can see the labels you added above.

Using 'kubectl create' to create the deployment is really useful, and a great tool for learning and creating the required yaml for all artefacts in the cluster.

Once you start heading to production though, you'll be wanting to automate deployment by creating, and deploying the yaml files (as we did in the earlier lab). Using tools such as Helm and Kustomize can make light work of templating/deploying applications into production.  

If you choose to store the yaml files into a Git repository, you can then starting thinking about gitops. Tools such as ArgoCD can be used to manage and deploy applications into production based on git repositories, and the changes that occur to the files held there.
