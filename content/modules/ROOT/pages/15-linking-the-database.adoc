The back end rest application is running and is accessible to the kubernetes cluster via its service definition `nationalparks-py`. 

The national parks data is held in a mongodb database which you deployed as the very first step in this workshop.

This database is useful as it ensures that the data is available to all replicas of the national parks rest service and of course is available to any other application requiring national parks information.

To view the resources for the database, run:

[.console-input]
[source,execute]
----
kubectl get deployment,service -l role=database -o name
----

This should yield:

[.console-output]
[source]
----
deployment.apps/mongodb-community-server
service/mongodb-community-server
----

You should understand now the purpose of `deployment` and `service`. 


********
The `persistentvolumeclaim` is used to claim persistent storage for use by the database. The `secret` is used to hold the credentials for the database.
********

To make this step of the workshop execute successfully, you need to deploy a slightly different version of the nationalparks-py back end service.

type:
[.console-input]
[source,execute]
----
kubectl apply -f backend-v2/
----

If you go back to parksmap application in yout browser and refresh the screen, you should notice that the application appears to be hanging, similar to the picture below.

image::broken-parksmap.png[Broken Parksmap, 900]

You could use what you've learned to try and debug the problem.

For example, you could try looking at the nationalpark-py pods logs to see if there are any clues there.

You could try running the `env` coommand in the nationalparks-py pod to see if there are any clues there.


In order to link the database to the back end rest application, you need to tell the back end rest application the name of the host for the database, and what the login credentials are. 

This is a common problem for any application. The need to provide environment specific configuration. In the database example you can be pretty confident that the database host, credentials, and possibly database name will be different between environments. To avoid hardcoding these and having to make application changes between environments we can use:

* Environment variables (defined in the deployment)
* Configmaps (externalised configuration)
* Secrets (externalised configuration that can be store in a secret vault)

Externalising configuration is best, storing the configuration in a seperate yaml file means that the deployment doesn't need to change between environments.

In this module, you'll take a look at both enironment variables and secrets. In terms of useage, Configmaps are similar to Secrets.

To start with you need to add environment variable settings to the `deployment` for the back end rest application. You can see what environment variables are already set for the back end rest application by running: web application.

The way that the back end rest application is implemented, it is expecting the following environment variables for a separate database.

* `DB_URI` - The host name of the database.
* `DATABASE-USER` - The user to login into the database.
* `DATABASE-PASSWORD` - The password of the user for the database.
* `DATABASE-NAME` - The name of the database.

To set environment variables, `kubectl` provides the command `kubectl set env`. You are going to update the live configuration change as you'll want to see these commands in action, but keep in mind that best practice would be to modify the locally stored deployment yaml file to enable the change to easily flow between enironments. Of course, you could add `--dry-run=client` to the command to capture the setting to assist in getting it right.

For the database host, the host name will be the name of the database `service` object, which is `mongodb-community-server`.

To see what the `deployment` configuration would look like with that set, we can run:

[.console-input]
[source,execute]
----
kubectl set env deployment/nationalparks-py DB_URI=mongodb-community-server -o yaml
----

You should see the following output:

[.console-output]
[source]
----
deployment.apps/nationalparks-py env updated
----

You can see what environment variables are set for the back end rest application by running:

[.console-input]
[source,execute]
----
kubectl set env deployment/nationalparks-py --list
----

The output should like this:
[.console-output]
[source]
----
DB_URI=mongodb-community-server
----

If you are unsure if this has worked successfully, you can execute a command in the running container to print the environment variable. We learned this earlier.

[.console-input]
[source,execute]
----
POD=`kubectl get pod -l component=nationalparks-py -o template --template '{{range .items}}{{.metadata.name}}{{"\n"}}{{end}}' | head -1` && echo $POD
----

[.console-input]
[source,execute]
----
kubectl exec $POD -- env | grep DB_URI
----

The output should like

[.console-output]
[source]
----
DB_URI=mongodb-community-server
----

This tells you that running the `kubectl set env` command has successfully update the deployment, resulting in a rolling update (default behaviour) of the pods being restarted to set the environment variable `DB_URI` to the value of `mongodb-community-server`. If we have more than one replica running, a rolling update ensures that the pods are updated one at a time resulting in no disruption in service availability.

The database credentials could be added in a similar way, but for this application we are going to use a `secret`. This makes sense as we potentially want to store sensitive information such as database credentials in a separate place, not expose them directly in the deployment definition and potentially lock them away in a secrets vault. You can view the `secret` called mongo-nationalparks-user-secret.yaml`:

[.console-input]
[source,execute]
----
cat apps/backend-v2/mongo-nationalparks-user-secret.yaml
----

Within the output you will see the `data` section holding values:

[.console-output]
[source]
----
apiVersion: v1
kind: Secret
metadata:
  name: mongodb-nationalparks-user
  labels:
    component: nationalparks-py
data:
  DATABASE_PASSWORD: bW9uZ29kYg==
  DATABASE_USER: bW9uZ29kYg==
type: Opaque
----

What you see aren't the actual values as they have been obfuscated using base64 encoding.

Now, add the secert to our Kubernetes cluster.

[.console-input]
[source,execute]
----
kubectl apply -f backend-v2/mongo-nationalparks-user-secret.yaml
----

Now take a look at the secret strored in the cluster.

In order to use the same values, but not actually have to copy them, you can configure the `deployment` to inject the environment variables from the secret. To see how the configuration should look for this you can run:

[.console-input]
[source,execute]
----
kubectl set env deployment/nationalparks-py --from secret/mongodb-nationalparks-user --dry-run=client -o yaml
----

For these, the `spec.template.spec.containers.env` setting would need to be updated to:

[.console-input]
[source,execute]
----
    spec:
      containers:
      - env:
        - name: DB_URI
          value: mongodb-community-server
        - name: DATABASE_PASSWORD
          valueFrom:
            secretKeyRef:
              key: DATABASE_PASSWORD
              name: mongodb-nationalparks-user
        - name: DATABASE_USER
          valueFrom:
            secretKeyRef:
              key: DATABASE_USER
              name: mongodb-nationalparks-user
----

You can see how the `env` will be changed to extract values from the secret.


Apply the change by executing the following command (not forgetting that best practise would be to update the deployment yaml file):

[.console-input]
[source,execute]
----
kubectl set env deployment/nationalparks-py --from secret/mongodb-nationalparks-user
----

You should see the following output:

[.console-input]
[source,execute]
----
Warning: key database-password transferred to DATABASE_PASSWORD
Warning: key database-user transferred to DATABASE_USER
deployment.apps/nationalparks-py env updated
----

The database and the back end rest application are now linked. Make sure everything is works by looking at Parksmap in the web browser again (you might need to resfresh the page).