To provide persistence for data you are using a MongoDB database, all instances of the application are using the same database, as we scale the application up and down, the database data is available to all instances. 

Currently, the databases data is stored inside the running container, the storage in the container is ephemeral, when a  database `pod` is killed, the data is lost. Not really the desired outcome of a database.

Lets test this out.

In your terminal window, type:

[.console-input]
[source,execute]
----
kubectl delete pod --force -l role=database
----

Go back to the Parksmap application in your browser. You should see that all the National Parks are gone. 

A resource that can be associated with any 'pod' is a persistent volume. The data stored in the persistent volume will survive when the pod is killed and restarted. `Persistent volumes` are typically stored in some form of shared storage and is configured by the kubernetes platform owner. You define the configuration in the template section of the deployment yaml file. It will then roll the configuration out to the resulting `pods`

To add persistent storage to an application, the first step is that you need to create a `persistent volume claim`. This tells Kubernetes that you need storage, how big the volume needs to be and what type of storage is required.

The resource definition for the persistent volume claim we need to use can be seen by running:

[.console-input]
[source,execute]
----
cat database-v1/persistentvolumeclaim.yaml
----

You should see that it contains:

[.console-output]
[source]
----
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: national-parks-database
  labels:
    app: national-parks-database
    role: database
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
----

This says that the storage should be at least of size 1Gi and that the access mode should be `ReadWriteOnce`.

Kubernetes supports three different access modes for storage.

* `ReadWriteOnce` (RWO) - The volume can be mounted as read/write by a single node.
* `ReadOnlyMany` (ROX) - The volume can be mounted as read-only by many nodes.
* `ReadWriteMany` (RWX) - The volume can be mounted as read/write by many nodes.

What access modes for storage are available will depend on the Kubernetes cluster.

For the database we are using, because we want to be able to run a single instance, we need storage with access mode `ReadWriteOnce`. 

Create the persistent volume claim by running the following command:

[.console-input]
[source,execute]
----
kubectl apply -f database-v1/persistentvolumeclaim.yaml
----

You should get confirmation telling you that the persistent volume claim has been created.

If you want to use some of the commands you've learned to confirm the creation then please feel free to.

In addition to creating the persistent volume claim, the `deployment` needs to be updated.

At `spec.template.spec.volumes` in the `deployment` we need to add:

[.console-output]
[source]
----
      volumes:
      ....
        - name: national-parks-database
          persistentVolumeClaim:
            claimName: national-parks-database
      ....
----

This indicates the persistent volume claim is to be used.

At `spec.template.spec.containers.volumeMounts`, we need to add:

[.console-output]
[source]
----
        volumeMounts:
        ....
          volumeMounts:
            - name: national-parks-database
              mountPath: /data/configdb
            - name: national-parks-database
              mountPath: /data/db
        ....
----

This indicates that the persistent volume should be mounted at the path `/data` in the container.

To see the updated `deployment` configuration run:

[.console-input]
[source,execute]
----
cat database-v1/deployment.yaml
----

To apply the configuration changes run:

[.console-input]
[source,execute]
----
kubectl apply -f database-v1/deployment.yaml
----

You should see confirmation that the deployment was configured.

The database should now be able to recover from failures, restarts and so on.

Test it out.

Firstly, lets load the datab back info the database. In the terminal window, run 

`curl http://nationalparks-py.wksp-{user}.svc.cluster.local:8080/ws/data/load`

Refresh the parksmap page in the `browser`. You should see national parks appearing on the map.

Now, kill the database pod again by running

[.console-input]
[source,execute]
----
kubectl delete pod --force -l role=database
----

NOTE: you are using the --force flag to terminate the pod quickly. Delete pod is a controlled shutdown which in our case means the terminating pod holds on to a mongodb lock file in the persistent volume. Without force, it will cause the new starting pod to fail as it cannot get a hold of the lock file.

Check the status of the pod to make sure it is running by typing the following into the terminal window:

[.console-input]
[source,execute]
----
kubectl get pods -l role=database
----

